{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/4bKRj0wffvazYlpfKgOL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/subarna007/streamlitProjects/blob/main/Week_3_Lab_Report_Information_Retrieval_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Importing Required Libraries"
      ],
      "metadata": {
        "id": "HmFmaHgPOewO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files  # For file uploads in Google Colab\n",
        "import re                      # For regex-based tokenization\n",
        "import math                    # For log calculations (IDF)\n",
        "from typing import List, Dict, Tuple"
      ],
      "metadata": {
        "id": "ZgEsHe8cOjca"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Uploading and Reading Documents"
      ],
      "metadata": {
        "id": "LOvpjA-QOzIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uploading files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Reading documents into a list\n",
        "documents = []\n",
        "for filename in uploaded.keys():\n",
        "    with open(filename, 'r', encoding='utf-8') as file:\n",
        "        documents.append(file.read())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TZNKd2bAOvgz",
        "outputId": "599029c9-7f99-42f7-8b0a-d71960841685"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bb5efadc-8664-4cd5-be37-62df4ffb92eb\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bb5efadc-8664-4cd5-be37-62df4ffb92eb\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Apple Mac Creator Dies.txt to Apple Mac Creator Dies (1).txt\n",
            "Saving argonaut_games_revival.txt to argonaut_games_revival (1).txt\n",
            "Saving Art Search Technology.txt to Art Search Technology (1).txt\n",
            "Saving BBC Interactive Bafta Wins.txt to BBC Interactive Bafta Wins (1).txt\n",
            "Saving Blog Popularity in the US.txt to Blog Popularity in the US (1).txt\n",
            "Saving bush_site_geo_block.txt to bush_site_geo_block (1).txt\n",
            "Saving christmas_gadget_trends.txt to christmas_gadget_trends (1).txt\n",
            "Saving Color-to-Music Software.txt to Color-to-Music Software (1).txt\n",
            "Saving decline_of_home_phones.txt to decline_of_home_phones (1).txt\n",
            "Saving doom_3_golden_joystick.txt to doom_3_golden_joystick (1).txt\n",
            "Saving Employee Monitoring Software.txt to Employee Monitoring Software (1).txt\n",
            "Saving EU Software Laws Delay.txt to EU Software Laws Delay (1).txt\n",
            "Saving Half-Life 2 Bafta Awards.txt to Half-Life 2 Bafta Awards (1).txt\n",
            "Saving high_definition_dvd.txt to high_definition_dvd (1).txt\n",
            "Saving id_fraud_shadowcrew_arrests.txt to id_fraud_shadowcrew_arrests (1).txt\n",
            "Saving Internet Regulation Debate.txt to Internet Regulation Debate (1).txt\n",
            "Saving Microsoft Anti-Piracy Move.txt to Microsoft Anti-Piracy Move (1).txt\n",
            "Saving Microsoft Search Engine.txt to Microsoft Search Engine (1).txt\n",
            "Saving Microsoft Word Security Warning.txt to Microsoft Word Security Warning (1).txt\n",
            "Saving microsoft_google_search_war.txt to microsoft_google_search_war (1).txt\n",
            "Saving Mobile Phone Chip.txt to Mobile Phone Chip (1).txt\n",
            "Saving mobile_games_evolution.txt to mobile_games_evolution (1).txt\n",
            "Saving Motion-Sensitive Mobile Phone.txt to Motion-Sensitive Mobile Phone (1).txt\n",
            "Saving movie_piracy_parental_software.txt to movie_piracy_parental_software (1).txt\n",
            "Saving MP3 Players in the US.txt to MP3 Players in the US (1).txt\n",
            "Saving napster_lawsuit_norway.txt to napster_lawsuit_norway (1).txt\n",
            "Saving new_internet_domains.txt to new_internet_domains (1).txt\n",
            "Saving Nintendo DS Launch.txt to Nintendo DS Launch (1).txt\n",
            "Saving ps2_slim_sales_boom.txt to ps2_slim_sales_boom (1).txt\n",
            "Saving robot_social_behavior.txt to robot_social_behavior (1).txt\n",
            "Saving Search Engine Habits.txt to Search Engine Habits (1).txt\n",
            "Saving Sony PSP.txt to Sony PSP (1).txt\n",
            "Saving spanish_rap_radio.txt to spanish_rap_radio (1).txt\n",
            "Saving Sports Games Rivalry.txt to Sports Games Rivalry (1).txt\n",
            "Saving star_wars_battlefront_review.txt to star_wars_battlefront_review (1).txt\n",
            "Saving tech_trends_2005.txt to tech_trends_2005 (1).txt\n",
            "Saving tsunami_aid_online_donations.txt to tsunami_aid_online_donations (1).txt\n",
            "Saving tv_loyalty_cards.txt to tv_loyalty_cards (1).txt\n",
            "Saving windows_atm_virus_risk.txt to windows_atm_virus_risk (1).txt\n",
            "Saving Xbox 2 Unveiling.txt to Xbox 2 Unveiling (1).txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Preprocessing Functions"
      ],
      "metadata": {
        "id": "m71apBcIPJkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop words list\n",
        "stop_words = {'the', 'is', 'in', 'and', 'of', 'a', 'to', ...}\n",
        "\n",
        "def pre_process(text: str) -> List[str]:\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenize\n",
        "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
        "    # Removing stop words\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    # Stemming\n",
        "    stemmed_tokens = []\n",
        "    for word in filtered_tokens:\n",
        "        if word.endswith('ing') and len(word) > 4:\n",
        "            word = word[:-3]\n",
        "        elif word.endswith('ed') and len(word) > 3:\n",
        "            word = word[:-2]\n",
        "        stemmed_tokens.append(word)\n",
        "    return stemmed_tokens"
      ],
      "metadata": {
        "id": "nSyHtXmVPG8P"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Computing Term Frequencies (TF)"
      ],
      "metadata": {
        "id": "G56Oj4R7PWRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_tf(doc_tokens: List[str]) -> Dict[str, int]:\n",
        "    tf = {}\n",
        "    for term in doc_tokens:\n",
        "        tf[term] = tf.get(term, 0) + 1\n",
        "    return tf\n",
        "\n",
        "# Applying to all documents\n",
        "processed_docs = [pre_process(doc) for doc in documents]\n",
        "docs_tf = [compute_tf(tokens) for tokens in processed_docs]"
      ],
      "metadata": {
        "id": "sWNQlke4O5XM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Computing Document Frequencies (DF)"
      ],
      "metadata": {
        "id": "tf3KjT2nPcL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_df(docs_tf: List[Dict[str, int]]) -> Dict[str, int]:\n",
        "    df = {}\n",
        "    for tf_dict in docs_tf:\n",
        "        for term in tf_dict.keys():\n",
        "            df[term] = df.get(term, 0) + 1\n",
        "    return df\n",
        "\n",
        "docs_df = compute_df(docs_tf)"
      ],
      "metadata": {
        "id": "D0SDuQTvPgbn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Computing TF-IDF Vectors"
      ],
      "metadata": {
        "id": "DVk20bauPjNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_tfidf_vector(tf: Dict[str, int], df: Dict[str, int], N: int) -> Dict[str, float]:\n",
        "    tfidf = {}\n",
        "    for term, freq in tf.items():\n",
        "        if term in df and df[term] != 0:\n",
        "            idf = math.log(N / df[term])  # IDF formula\n",
        "            tfidf[term] = freq * idf\n",
        "    # Normalizing with L2 norm\n",
        "    norm = math.sqrt(sum(val**2 for val in tfidf.values()))\n",
        "    return {term: val/norm for term, val in tfidf.items()}\n",
        "\n",
        "N = len(docs_tf)\n",
        "tfidf_vectors = [compute_tfidf_vector(tf, docs_df, N) for tf in docs_tf]"
      ],
      "metadata": {
        "id": "ur7ZvNm1Ph0E"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Query Processing & Ranking"
      ],
      "metadata": {
        "id": "54Is_0NoPquO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rank_documents(query: str, docs: List[str], k: int) -> List[Tuple[int, float]]:\n",
        "    # Preprocessing query\n",
        "    query_tokens = pre_process(query)\n",
        "    query_tf = compute_tf(query_tokens)\n",
        "    query_tfidf = compute_tfidf_vector(query_tf, docs_df, len(docs))\n",
        "\n",
        "    # Computing cosine similarity with each document\n",
        "    similarities = []\n",
        "    for i, doc_vector in enumerate(tfidf_vectors):\n",
        "        dot_product = sum(\n",
        "            query_tfidf.get(term, 0.0) * doc_vector.get(term, 0.0)\n",
        "            for term in query_tfidf\n",
        "        )\n",
        "        similarities.append((i, dot_product))\n",
        "\n",
        "    # Returning top-k results\n",
        "    return sorted(similarities, key=lambda x: x[1], reverse=True)[:k]"
      ],
      "metadata": {
        "id": "OcgWtN_tPt8L"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example Query:"
      ],
      "metadata": {
        "id": "2B8XMT3uQd-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ranked_docs = rank_documents(\"high income\", documents, k=3)\n"
      ],
      "metadata": {
        "id": "9lUGjuY3Pxuz"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8: Experimentation & Results"
      ],
      "metadata": {
        "id": "eBhVtHLuRAVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test queries\n",
        "queries = [\"Liquid helium\", \"Gradient System\"]\n",
        "for query in queries:\n",
        "    results = rank_documents(query, documents, k=3)\n",
        "    print(f\"Query: '{query}'\")\n",
        "    for rank, (idx, score) in enumerate(results, 1):\n",
        "        print(f\"{rank}. Doc {idx+1} (Score: {score:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HUYh105RB7r",
        "outputId": "75cf2997-2f71-4938-d033-e74eb1755e04"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: 'Liquid helium'\n",
            "1. Doc 1 (Score: 0.0000)\n",
            "2. Doc 2 (Score: 0.0000)\n",
            "3. Doc 3 (Score: 0.0000)\n",
            "Query: 'Gradient System'\n",
            "1. Doc 11 (Score: 0.2076)\n",
            "2. Doc 17 (Score: 0.0822)\n",
            "3. Doc 39 (Score: 0.0790)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 9: Experiment Setup"
      ],
      "metadata": {
        "id": "0Hv_z2eKRmiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "queries = [\n",
        "    \"More growth in the market is inevitable as new devices become available\",\n",
        "    \"Microsoft has unveiled the finished version of its home-grown search engine\",\n",
        "    \"The ESPN games are a touch more arcade-like in look and feel and are slightly easier to get into\",\n",
        "    \"Moves to unite mobile and fixed phones look set to get more emphasis in 2005 too\"\n",
        "]"
      ],
      "metadata": {
        "id": "tGf29e6_RpiV"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running Retrieval Pipeline"
      ],
      "metadata": {
        "id": "--qKBh6tSEIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "for query in queries:\n",
        "    ranked_docs = rank_documents(query, documents, k=3)  # Added documents and k=3\n",
        "    results[query] = ranked_docs"
      ],
      "metadata": {
        "id": "uzBPwbQuSBQt"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 10: Report Results"
      ],
      "metadata": {
        "id": "o8jop1NXSNdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for query, ranked_docs in results.items():\n",
        "    print(f\"\\nQuery: '{query}'\")\n",
        "    print(\"Top 3 Documents:\")\n",
        "    for rank, (doc_idx, score) in enumerate(ranked_docs, 1):\n",
        "        doc_preview = documents[doc_idx][:100].replace('\\n', ' ')  # Showing first 100 chars\n",
        "        print(f\"{rank}. Doc {doc_idx + 1} (Score: {score:.4f}): {doc_preview}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cu21vR5OSHUQ",
        "outputId": "b847b049-71cb-4e7f-93a9-fe12d9ac3bfa"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Query: 'More growth in the market is inevitable as new devices become available'\n",
            "Top 3 Documents:\n",
            "1. Doc 25 (Score: 0.1669): Millions buy MP3 players in US  One in 10 adult Americans - equivalent to 22 million people - owns a...\n",
            "2. Doc 5 (Score: 0.0756): Blog reading explodes in America  Americans are becoming avid blog readers, with 32 million getting ...\n",
            "3. Doc 36 (Score: 0.0751): The future in your pocket  If you are a geek or gadget fan, the next 12 months look like they are go...\n",
            "\n",
            "Query: 'Microsoft has unveiled the finished version of its home-grown search engine'\n",
            "Top 3 Documents:\n",
            "1. Doc 18 (Score: 0.5105): Microsoft launches its own search  Microsoft has unveiled the finished version of its home-grown sea...\n",
            "2. Doc 20 (Score: 0.2543): Search wars hit desktop PCs  Another front in the on-going battle between Microsoft and Google is ab...\n",
            "3. Doc 31 (Score: 0.1923): Savvy searchers fail to spot ads  Internet search engine users are an odd mix of naive and sophistic...\n",
            "\n",
            "Query: 'The ESPN games are a touch more arcade-like in look and feel and are slightly easier to get into'\n",
            "Top 3 Documents:\n",
            "1. Doc 34 (Score: 0.3840): Sporting rivals go to extra time  The current slew of sports games offers unparalleled opportunities...\n",
            "2. Doc 22 (Score: 0.0836): Mobile games come of age  The BBC News website takes a look at how games on mobile phones are maturi...\n",
            "3. Doc 35 (Score: 0.0613): The Force is strong in Battlefront  The warm reception that has greeted Star Wars: Battlefront is a ...\n",
            "\n",
            "Query: 'Moves to unite mobile and fixed phones look set to get more emphasis in 2005 too'\n",
            "Top 3 Documents:\n",
            "1. Doc 9 (Score: 0.2788): Home phones face unclear future  The fixed line phone in your home could soon be an endangered speci...\n",
            "2. Doc 36 (Score: 0.2320): The future in your pocket  If you are a geek or gadget fan, the next 12 months look like they are go...\n",
            "3. Doc 21 (Score: 0.1126): Cheaper chip for mobiles  A mobile phone chip which combines a modem and a computer processor on one...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FTtW1-m0SSN-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}